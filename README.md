# Wikipedia Document Comparison System

Система автоматического сравнения документов Wikipedia с использованием семантического поиска и локальной LLM через Ollama.

## О проекте

Данный проект представляет собой **демонстрацию универсального пайплайна** для автоматического сравнения документов с использованием современных технологий NLP. Основная цель - показать, как можно построить end-to-end решение для семантического анализа и сравнения текстов.

### Ключевые особенности пайплайна:
- **Модульность**: каждый компонент может быть заменен или доработан независимо
- **Универсальность**: легко адаптируется под разные источники данных и модели
- **Масштабируемость**: поддерживает кэширование и оптимизацию производительности
- **Гибкость**: настраиваемые параметры для разных задач и ресурсов

Хотя в примере используется `gemma3:27b`, система спроектирована так, чтобы работать с любыми локальными моделями через Ollama или даже другие способы запуска LLM.

## Архитектура системы

```
┌─────────────────────┐    ┌──────────────────────┐    ┌─────────────────────┐
│  Wikipedia URLs     │───▶│   Text Chunking      │───▶│   Embeddings        │
│  (2 articles)       │    │   (wikipedia_        │    │   (create_          │
│                     │    │    chunker.py)       │    │    embeddings.py)   │
└─────────────────────┘    └──────────────────────┘    └─────────────────────┘
                                       │                          │
                                       ▼                          ▼
┌─────────────────────┐    ┌──────────────────────┐    ┌─────────────────────┐
│   User Query        │───▶│   Semantic Search    │◄───│   Stored Embeddings │
│                     │    │   (query_embeddings  │    │   (.npz files)      │
│                     │    │    .py)              │    │                     │
└─────────────────────┘    └──────────────────────┘    └─────────────────────┘
                                       │
                                       ▼
┌─────────────────────┐    ┌──────────────────────┐
│   LLM Comparison    │◄───│   Relevant Chunks    │
│   via Ollama        │    │   from both docs     │
│   (compare_docs.py) │    │                      │
└─────────────────────┘    └──────────────────────┘
```

## Файлы системы

### 1. `wikipedia_chunker.py`
**Назначение**: Извлечение и структурирование контента из статей Wikipedia

**Основной класс**: `WikipediaChunker`

**Ключевые методы**:
- `get_chunks_from_url(url)` - извлечение и разбиение одной статьи на фрагменты
- `get_chunks_from_multiple_urls(urls)` - обработка нескольких URL одновременно
- `_extract_paragraph_chunks(soup)` - основная логика разбиения на параграфы
- `_split_large_paragraph(text)` - разделение больших параграфов на меньшие части
- `_process_section_paragraphs()` - объединение малых фрагментов при необходимости

**Особенности**:
- Сохраняет иерархию заголовков (h1-h6)
- Поддерживает метаданные для каждого фрагмента
- Автоматически объединяет короткие параграфы
- Разбивает слишком длинные тексты по границам предложений
- Фильтрует служебную информацию (навигация, таблицы, ссылки)

**Параметры конфигурации**:
- `min_paragraph_chars` (по умолчанию: 100) - минимальная длина фрагмента
- `max_paragraph_chars` (по умолчанию: 3000) - максимальная длина фрагмента
- `combine_small_paragraphs` (по умолчанию: True) - объединять ли короткие параграфы

### 2. `create_embeddings.py`
**Назначение**: Создание векторных представлений текста с использованием модели RetroMAE

**Основной класс**: `RetroMAEEmbedder`

**Ключевые методы**:
- `get_embeddings(texts, batch_size=8)` - создание эмбеддингов для списка текстов
- `get_optimal_transport_embedding()` - вычисление OT-эмбеддинга из токенных представлений
- `process_wikipedia_chunks()` - обработка фрагментов Wikipedia и генерация эмбеддингов

**Стратегии объединения эмбеддингов**:
- `"concat"` (по умолчанию) - конкатенация CLS и OT эмбеддингов
- `"weighted"` - взвешенное объединение через проекционные слои

**Вспомогательные функции**:
- `clean_filename(filename)` - создание безопасного имени файла
- `extract_title_from_url(url)` - извлечение заголовка из URL Wikipedia

**Технические особенности**:
- Автоматическое определение устройства (CUDA/CPU)
- Батчевая обработка для оптимизации памяти
- Максимальная длина токенов: 512
- Сохранение в формате .npz с метаданными

### 3. `query_embeddings.py`
**Назначение**: Семантический поиск по созданным эмбеддингам

**Основной класс**: `SemanticSearcher`

**Ключевые методы**:
- `search(urls, query_text, top_k=3)` - поиск релевантных фрагментов по URL
- `load_embeddings(url)` - загрузка сохраненных эмбеддингов
- `get_query_embedding(query_text)` - создание эмбеддинга для поискового запроса
- `find_relevant_chunks()` - поиск наиболее похожих фрагментов
- `url_to_filename(url)` - преобразование URL в имя файла эмбеддингов

**Алгоритм поиска**:
1. Создание эмбеддинга для поискового запроса
2. Вычисление косинусного сходства между запросом и всеми фрагментами
3. Сортировка по убыванию сходства
4. Возврат top-k наиболее релевантных результатов

**Вспомогательные функции**:
- `display_results(results)` - форматированный вывод результатов поиска

### 4. `compare_documents.py`
**Назначение**: Автоматическое сравнение документов с использованием LLM

**Основной класс**: `AutomatedDocumentComparer`

**Ключевые методы**:
- `compare_documents(urls, query, top_k=3)` - основной метод сравнения двух документов
- `ensure_embeddings_exist(urls)` - проверка и создание эмбеддингов при необходимости
- `create_embeddings_for_url(url)` - создание эмбеддингов для одного URL
- `query_ollama(prompt)` - отправка запроса к локальной LLM
- `create_comparison_prompt()` - формирование промпта для сравнения

**Процесс сравнения**:
1. Проверка существования эмбеддингов для обеих статей
2. Создание недостающих эмбеддингов при необходимости
3. Семантический поиск релевантных фрагментов в каждом документе
4. Формирование промпта с найденными фрагментами
5. Генерация сравнительного анализа через Ollama
6. Возврат структурированного результата

**Параметры конфигурации**:
- `model_name` (по умолчанию: "gemma3:27b") - любая модель Ollama
- `temperature` (по умолчанию: 0.0) - температура генерации
- `embeddings_dir` (по умолчанию: "wikipedia_embeddings") - директория для эмбеддингов

**Примечание**: Система легко адаптируется под другие способы запуска LLM. Для интеграации с другими API достаточно модифицировать метод `query_ollama()`.

## Установка и настройка

### Требования
- Python 3.8+
- Ollama (установленный и запущенный локально)
- Все Python зависимости указаны в `requirements.txt`

### Установка зависимостей
```bash
pip install -r requirements.txt
```

### Установка и настройка Ollama
```bash
# Скачайте и установите Ollama с официального сайта: https://ollama.ai
# Запустите Ollama:
ollama serve

# Загрузите нужную модель (примеры):
ollama pull gemma3:27b      # Большая модель (рекомендуется 32GB+ RAM)
ollama pull llama3.1:8b     # Средняя модель (рекомендуется 16GB+ RAM)  
ollama pull phi3:mini       # Компактная модель (подойдет для 8GB+ RAM)
```

**Примечание**: `gemma3:27b` - это демонстрационная модель для показа возможностей системы. Вы можете использовать любую другую локальную модель, доступную в Ollama, изменив параметр `--model`. Система спроектирована как универсальный пайплайн, который легко адаптируется под разные модели и даже другие способы запуска LLM (не только через Ollama).

## Использование

### Командная строка
```bash
python compare_documents.py \
    --urls "https://en.wikipedia.org/wiki/Machine_Learning" \
           "https://en.wikipedia.org/wiki/Artificial_Intelligence" \
    --query "applications in healthcare" \
    --top_k 3 \
    --model "llama3.1:8b" \  # Можете использовать любую доступную модель
    --verbose
```

### Параметры командной строки
- `--urls` - два URL Wikipedia для сравнения (обязательно)
- `--query` - поисковый запрос (обязательно)
- `--top_k` - количество релевантных фрагментов на документ (по умолчанию: 3)
- `--model` - название модели Ollama (по умолчанию: "gemma3:27b", можете использовать любую доступную)
- `--temperature` - температура генерации (по умолчанию: 0.0)
- `--embeddings_dir` - директория для сохранения эмбеддингов
- `--save_prompt` - сохранить сгенерированный промпт в файл
- `--verbose` - показать подробные результаты поиска

### Программное использование
```python
from compare_documents import AutomatedDocumentComparer

comparer = AutomatedDocumentComparer(
    model_name="llama3.1:8b",  # Используйте модель по своему выбору
    temperature=0.0
)

urls = [
    "https://en.wikipedia.org/wiki/Machine_Learning",
    "https://en.wikipedia.org/wiki/Artificial_Intelligence"
]

results = comparer.compare_documents(
    urls=urls,
    query="applications in healthcare",
    top_k=3
)

print(results["comparison"])
```

## Структура выходных данных

### Результат сравнения содержит:
```python
{
    "embeddings_status": {
        "url1": {"status": "exists|created|error", ...},
        "url2": {"status": "exists|created|error", ...}
    },
    "search_results": {
        "url1": {
            "title": "Article Title",
            "chunks": [
                {
                    "text": "relevant text fragment",
                    "metadata": {...},
                    "similarity_score": 0.85
                }
            ]
        }
    },
    "comparison": "Сгенерированное LLM сравнение на русском языке",
    "prompt_used": "Полный промпт, отправленный в LLM"
}
```

### Метаданные фрагментов включают:
- `title` - заголовок статьи
- `section_level` - уровень секции (introduction, h1-h6)
- `h1`-`h6` - иерархия заголовков
- `element_type` - тип HTML-элемента
- `part` - номер части (если фрагмент был разделен)

## Технические особенности

### Стратегии эмбеддингов
Система поддерживает две стратегии объединения CLS и OT эмбеддингов:
1. **Конкатенация** - простое объединение векторов
2. **Взвешенное объединение** - использование обученных проекционных слоев

### Оптимизация производительности
- Батчевая обработка текстов при создании эмбеддингов
- Кэширование эмбеддингов в .npz файлах
- Автоматическое определение GPU/CPU
- Повторное использование созданных эмбеддингов

### Обработка ошибок
- Проверка доступности URL Wikipedia
- Валидация формата эмбеддингов
- Обработка ошибок подключения к Ollama
- Информативные сообщения об ошибках

## Примеры использования

### Сравнение научных концепций
```bash
python compare_documents.py \
    --urls "https://en.wikipedia.org/wiki/Artificial_intelligence" \
           "https://en.wikipedia.org/wiki/Machine_learning" \
    --query "How do neural networks work in AI?"
```

### Анализ исторических событий
```bash
python compare_documents.py \
    --urls "https://en.wikipedia.org/wiki/World_War_I" \
           "https://en.wikipedia.org/wiki/World_War_II" \
    --query "causes and consequences" \
    --top_k 5
```

### Сравнение географических регионов
```bash
python compare_documents.py \
    --urls "https://en.wikipedia.org/wiki/Europe" \
           "https://en.wikipedia.org/wiki/Asia" \
    --query "economic development and trade"
```

## Структура файлов проекта

```
project/
├── compare_documents.py      # Основной модуль сравнения
├── create_embeddings.py      # Создание векторных представлений
├── query_embeddings.py       # Семантический поиск
├── wikipedia_chunker.py      # Парсинг и разбиение Wikipedia
├── wikipedia_embeddings/     # Директория для сохранения эмбеддингов
│   ├── Article_Name_concat_embeddings.npz
│   ├── Another_Article_concat_embeddings.npz
│   └── ...
├── comparison_prompt.txt     # Сохраненный промпт (опционально)
└── README.md                # Документация
```

### Ограничения и рекомендации

### Технические ограничения
- Максимальная длина токенов: 512 (ограничение RetroMAE)
- Требует запущенного экземпляра Ollama (или может быть адаптирован под другие LLM API)
- Зависит от доступности Wikipedia
- Обрабатывает только статьи Wikipedia (но архитектура легко расширяема на другие источники)

### Рекомендации по использованию
- Используйте конкретные и четкие поисковые запросы
- Для больших статей увеличьте значение `top_k`
- Регулируйте `temperature` для контроля креативности ответов LLM
- Сохраняйте эмбеддинги для повторного использования
- Выбирайте модель в соответствии с доступными ресурсами

### Производительность
- Первое создание эмбеддингов может занять несколько минут
- Повторные сравнения выполняются значительно быстрее благодаря кэшированию
- Рекомендуется GPU для ускорения создания эмбеддингов
- Время генерации ответа зависит от выбранной LLM модели

## Расширение и адаптация

### Интеграция с другими источниками данных
Система легко адаптируется для работы с другими источниками:
- Замените `WikipediaChunker` на парсер для вашего источника
- Сохраните тот же интерфейс методов (`get_chunks_from_url`, структура метаданных)

### Адаптация модели эмбеддингов
RetroMAE предобучена на англоязычной википедии, поэтому для конкретной задачи ее дообучение не требуется.
Но если в проекте используются собственные документы, то модель нужно дообучить на этих данных.

### Интеграция с другими LLM
Для использования не Ollama API:
- Измените метод `query_ollama()` в классе `AutomatedDocumentComparer`
- Адаптируйте формат запросов под ваш API (OpenAI, Anthropic, и т.д.)

## Поддержка и разработка

Система спроектирована как **демонстрационный пайплайн** и легко расширяема:
- Можно добавить поддержку других источников данных (не только Wikipedia)
- Возможна интеграция с другими моделями эмбеддингов
- Поддержка различных LLM (не только через Ollama)
- Настраиваемые стратегии постобработки результатов
- Модульная архитектура позволяет заменять любые компоненты независимо